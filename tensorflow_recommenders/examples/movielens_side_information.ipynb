{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmR0Ojuf6eHq"
   },
   "source": [
    "# Using side information in retrieval\n",
    "\n",
    "In the basic retrieval tutorial we built a simple retrieval model for the Movielens dataset, using user and movie ids as only features.\n",
    "\n",
    "However, it's often useful to use a richer set of features, for both queries and candidates. For example:\n",
    "\n",
    "1. In a large candidate catalogue, there may not be enough data per item to accurately estimate an embedding vector for every item. Using items features (categories, descriptions, images) will help build a model that's more accurate and generalizes better to unseen items.\n",
    "2. Some applications (like fashion) have candidate sets that change frequently, and any given item is available only for a short time. This leaves little time for learning of accurate item representations.\n",
    "3. Users' preferences may change depending on the context they are in. For example, a single user may prefer to consume short-form content when on their phone, and reserve long-form content for their TV. Being able to use the context of the interaction in the model will help capture these nuances.\n",
    "4. Items' relevance may change over time. Including time as an explicit feature will help the model capture popularity trends, preventing items that were once popular but not relevant any more from dominating future recommendations.\n",
    "\n",
    "In this example, we're going to make use of query context to improve our initial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBA9KfOdFa2i"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "As before, let's start with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WD2sAmES6dQB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as  tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kxv8x85QIHY1"
   },
   "source": [
    "We also need to repeat the training/test splits and vocabulary building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHBSvbkHITdu"
   },
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movie_lens/100k-movies\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"timestamp\": x[\"timestamp\"],\n",
    "})\n",
    "movies = movies.map(lambda x: {\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "})\n",
    "\n",
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "movie_ids = movies.batch(1_000_000).map(lambda x: x[\"movie_id\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_ids = np.unique(np.concatenate(list(movie_ids)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14rJMDmfxOT8"
   },
   "source": [
    "## Using query context information\n",
    "\n",
    "The first piece of side information we can make use of is the time when a rating is given. This will help us capture two things:\n",
    "\n",
    "1. Popularity dynamics: some movies are watched a lot when they are first released, but do not become classics. This makes them a good recommendation soon after release, but a bad recommendation later on.\n",
    "2. User tastes changing over time. Making sure our model has the capacity to express this via interactions of user embeddings and time will help us capture this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u117xGdgJS50"
   },
   "source": [
    "### Representing time\n",
    "\n",
    "Our ratings dataset has raw timestamp features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Xu0WwUV0JaRP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 879024327\n",
      "Timestamp: 875654590\n"
     ]
    }
   ],
   "source": [
    "for row in ratings.take(2).as_numpy_iterator():\n",
    "  print(f\"Timestamp: {row['timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhyRhTPQJtUw"
   },
   "source": [
    "We can't use this directly: we need normalized features in order to make our learning algorithm stable.\n",
    "\n",
    "There are many ways of doing this:\n",
    "1. We could treat time as a continuous feature, and scale it to lie between 0 and 1. This coule be done via quantile scaling (like in sklearn's [QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html)) or through any variety of other methods that map an arbitrary real variable to a fixed interval.\n",
    "2. We could treat time as a discrete variable: a series of discrete time periods.\n",
    "\n",
    "The advantage of treating time as a continuous variable is that its effects become easy to estimate: there are only a few parameters added to the model. However, given enough data, treating time as discrete gives us a more flexible model, where each period as different effects, rather than following a smooth curve over time.\n",
    "\n",
    "In this example, we're going to adopt the discrete approach. We'll start by dividing time into 1000 equal buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lkoxVjgVxg8P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: [8.74724710e+08 8.74743291e+08 8.74761871e+08]\n"
     ]
    }
   ],
   "source": [
    "max_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
    "min_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(np.int64(1e9), tf.minimum).numpy().min()\n",
    "\n",
    "timestamp_buckets = np.linspace(min_timestamp, max_timestamp, num=1000)\n",
    "\n",
    "print(f\"Buckets: {timestamp_buckets[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrXeuhFOPORV"
   },
   "source": [
    "### Query model\n",
    "\n",
    "We can now map those into embeddings via definig appropriate feature columns for our query model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xC-1HkpiPTzZ"
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 32\n",
    "\n",
    "#  An embedding column for user ids, as before.\n",
    "user_id_feature = tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            \"user_id\", unique_user_ids),\n",
    "        embedding_dimension)\n",
    "\n",
    "# An embedding column for the bucketized timestamps: there will be a separate\n",
    "# embedding for each of the timestamp buckets.\n",
    "time_feature = tf.feature_column.embedding_column(\n",
    "        tf.feature_column.bucketized_column(\n",
    "            tf.feature_column.numeric_column(\"timestamp\"),\n",
    "            timestamp_buckets.tolist()),\n",
    "        embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaDfwwsJQDOG"
   },
   "source": [
    "We can now use these features in the model. While we could use complex models here, let's start with something simple: we simply concatenate the time and user embedding, and project them onto the item embedding dimension (remember, the output dimensionality between the user model and the candidate model has to be the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKpA9-ytQ0A1"
   },
   "outputs": [],
   "source": [
    "user_model_input = {\n",
    "    \"user_id\": tf.keras.Input(shape=(), dtype=tf.string, name=\"user_id\"),\n",
    "    \"timestamp\": tf.keras.Input(shape=(), dtype=tf.int64, name=\"timestamp\"),\n",
    "}\n",
    "user_features = tf.keras.layers.DenseFeatures([user_id_feature, time_feature])(user_model_input)\n",
    "user_embedding = tf.keras.layers.Dense(embedding_dimension)(user_features)\n",
    "user_model = tf.keras.Model(user_model_input, user_embedding, name=\"user_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtD25VQfRaTO"
   },
   "source": [
    "### Candidate model\n",
    "\n",
    "Let's keep the candidate model as before - using id information only - and see what effect the inclusion of context information has on the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyCLTbDZRjNo"
   },
   "outputs": [],
   "source": [
    "movie_features = [tf.feature_column.embedding_column(\n",
    "  tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"movie_id\", list(unique_movie_ids)),\n",
    "  embedding_dimension)]\n",
    "\n",
    "movie_model_input = {\"movie_id\": tf.keras.Input(shape=(), dtype=tf.string)}\n",
    "movie_embedding = tf.keras.layers.DenseFeatures(movie_features)(movie_model_input)\n",
    "movie_model = tf.keras.Model(movie_model_input, movie_embedding, name=\"movie_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exbGTpplaq2e"
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "As before, we put it all together in a model class we'll use for fitting and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-O1DKkxB0E94"
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.task = tfrs.tasks.RetrievalTask(\n",
    "      corpus_metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates=movies.batch(128).map(movie_model)\n",
    "      )\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "\n",
    "    user_embeddings = self.user_model({\"user_id\": features[\"user_id\"],\n",
    "                                       \"timestamp\": features[\"timestamp\"]})\n",
    "    positive_movie_embeddings = self.movie_model(\n",
    "        {\"movie_id\": features[\"movie_id\"]})\n",
    "\n",
    "    return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aW63YaqP2wCf"
   },
   "outputs": [],
   "source": [
    "model = MovielensModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nma0vc2XdN5g"
   },
   "source": [
    "Then shuffle, batch, and cache the training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53QJwY1gUnfv"
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8mHTxKAdTJO"
   },
   "source": [
    "Then train the  model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "ZxPntlT8EFOZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 14s 1s/step - factorized_top_k: 0.0348 - factorized_top_k/top_1_categorical_accuracy: 8.8750e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0051 - factorized_top_k/top_10_categorical_accuracy: 0.0108 - factorized_top_k/top_50_categorical_accuracy: 0.0534 - factorized_top_k/top_100_categorical_accuracy: 0.1039 - loss: 69728.0987\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 15s 1s/step - factorized_top_k: 0.0791 - factorized_top_k/top_1_categorical_accuracy: 0.0018 - factorized_top_k/top_5_categorical_accuracy: 0.0122 - factorized_top_k/top_10_categorical_accuracy: 0.0258 - factorized_top_k/top_50_categorical_accuracy: 0.1235 - factorized_top_k/top_100_categorical_accuracy: 0.2324 - loss: 67110.0483\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 16s 2s/step - factorized_top_k: 0.1049 - factorized_top_k/top_1_categorical_accuracy: 0.0022 - factorized_top_k/top_5_categorical_accuracy: 0.0188 - factorized_top_k/top_10_categorical_accuracy: 0.0380 - factorized_top_k/top_50_categorical_accuracy: 0.1698 - factorized_top_k/top_100_categorical_accuracy: 0.2959 - loss: 65776.7060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7e3cb7acbb70>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uusQ4WcCgSCo"
   },
   "source": [
    "### Results\n",
    "\n",
    "What do the results look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "m6lZCalq1OwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 526ms/step - factorized_top_k: 0.0721 - factorized_top_k/top_1_categorical_accuracy: 7.0000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0076 - factorized_top_k/top_10_categorical_accuracy: 0.0163 - factorized_top_k/top_50_categorical_accuracy: 0.1137 - factorized_top_k/top_100_categorical_accuracy: 0.2222 - loss: 31004.3128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k': array([0.0007 , 0.00755, 0.0163 , 0.1137 , 0.2222 ], dtype=float32),\n",
       " 'factorized_top_k/top_1_categorical_accuracy': 0.000699999975040555,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.007550000213086605,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.016300000250339508,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.1137000024318695,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.22220000624656677,\n",
       " 'loss': 28181.4375}"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPBlwExfgU_A"
   },
   "source": [
    "Our accuracies for all values of k are quite a bit better than the model from the basic retrieval tutorial. Clearly, accounting for time is useful.\n",
    "\n",
    "When using timestamp information, the performance of the model can vary with the number of buckets we use when bucketizing the timestamps. In this tutorial we used 1000 buckets, which significantly improved the performance of the model. However, when using 100 buckets, the performance of the model using timestamp information would not be significantly better than the performance of the model from the basic retrieval tutorial."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "movielens_side_information.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
