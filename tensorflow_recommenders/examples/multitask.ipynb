{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCeYA79m1DEX"
   },
   "source": [
    "# Multi-task recommenders\n",
    "\n",
    "\n",
    "In the [Movielens retrieval tutorial](https://github.com/tensorflow/recommenders/examples/tfrs_movielens.ipynb) we built a retrieval system using movie watches as positive interaction signals.\n",
    "\n",
    "In many applications, however, there are multiple rich sources of feedback to draw upon. For example, an e-commerce site may record user visits to product pages (abundant, but relatively low signal), image clicks, adding to cart, and, finally, purchases. It may even record post-purchase signals such as reviews and returns.\n",
    "\n",
    "Integrating all these different forms of feedback is critical to building systems that users love to use, and that do not optimize for any one metric at the expense of overall performance.\n",
    "\n",
    "In this tutorial, we are going to build a multi-objective recommender for Movielens, using both implicit (movie watches) and explicit signals (ratings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZwrcZeK7x7xI"
   },
   "source": [
    "## Imports\n",
    "\n",
    "\n",
    "Let's first get our imports out of the way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZGYDaF-m5wZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as  tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxQ_hy7xPH3N"
   },
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PAqjR4a1RR4"
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "As before, we're going to use the Movielens 100K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ySWtibjm_6a"
   },
   "outputs": [],
   "source": [
    "ratings, movies = tfrs.datasets.movielens_100K()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRHorm8W1yf3"
   },
   "source": [
    "And repeat our preparations for building vocabularies and splitting the data into a train and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rS0eDfkjnjJL"
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "movie_ids = ratings.batch(100_000).map(lambda x: x[\"movie_id\"])\n",
    "user_ids = ratings.batch(100_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_ids = np.unique(np.concatenate(list(movie_ids)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCi-seR86qqa"
   },
   "source": [
    "## A multi-task model\n",
    "\n",
    "There are two critical parts to multi-task recommenders:\n",
    "\n",
    "1. They optimize for two or more objectives, and so have two or more losses.\n",
    "2. They share variables between the tasks, allowing for transfer learning.\n",
    "\n",
    "In this tutorial, we will define our models as before, but instead of having  a single task, we will have two tasks: one that predicts ratings, and one that predicts movie watches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXHrk_SLzKCM"
   },
   "source": [
    "The user and movie models are as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoRWPZ1RzPGB"
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 32\n",
    "\n",
    "# We wrap these in functions this time so that we can run multiple independent\n",
    "# models later.\n",
    "def user_model():\n",
    "  user_features = [tf.feature_column.embedding_column(\n",
    "          tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "              \"user_id\", unique_user_ids),\n",
    "          embedding_dimension)]\n",
    "  return tf.keras.Sequential([tf.keras.layers.DenseFeatures(user_features)])\n",
    "\n",
    "\n",
    "def movie_model():\n",
    "  movie_features = [tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      \"movie_id\", list(unique_movie_ids)),\n",
    "    embedding_dimension)]\n",
    "  return  tf.keras.Sequential([tf.keras.layers.DenseFeatures(movie_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWCwkE5z8QBe"
   },
   "source": [
    "However, now we will have two tasks. The first is the rating task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "id": "gg-mw6UNzZvs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.tasks.ranking.RankingTask at 0x7f61213b2978>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrs.tasks.RankingTask(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrgQIXEm8UWf"
   },
   "source": [
    "Its goal is to predict the ratings as accurately as possible.\n",
    "\n",
    "The second is the retrieval task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "id": "mT-nT6Cz8W8Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.tasks.retrieval.RetrievalTask at 0x7f6120f10358>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrs.tasks.RetrievalTask(\n",
    "  corpus_metrics=tfrs.metrics.FactorizedTopK(\n",
    "    candidates=movies.batch(128)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCNrv7_gakmF"
   },
   "source": [
    "As before, this task's goal is to predict which movies the user will or will not watch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSWw3xuq8mGh"
   },
   "source": [
    "### Putting it together\n",
    "\n",
    "We put it all together in a model class.\n",
    "\n",
    "The new component here is that - since we have two tasks and two losses - we need to decide on how important each loss is. We can do this by giving each of the losses a weight, and treating these weights as hyperparameters. If we assign a large loss weight to the rating task, our model is going to focus on predicting ratings (but still use some information from the retrieval task); if we assign a large loss weight to the retrieval task, it will focus on retrieval instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFSkOAMgzU0K"
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, rating_weight: float, retrieval_weight: float) -> None:\n",
    "    # We take the loss weights in the constructor: this allows us to instantiate\n",
    "    # several model objects with different loss weights.\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    # Embeddings as before.\n",
    "    self.movie_model: tf.keras.layers.Layer = movie_model()\n",
    "    self.user_model: tf.keras.layers.Layer = user_model()\n",
    "\n",
    "    # A small model to take in user and movie embeddings and predict ratings.\n",
    "    # We can make this as complicated as we want as long as we output a scalar\n",
    "    # as our prediction.\n",
    "    self.rating_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # The tasks.\n",
    "    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.RankingTask(\n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    "    )\n",
    "    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.RetrievalTask(\n",
    "      corpus_metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates=movies.batch(128).map(self.movie_model)\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # The loss weights.\n",
    "    self.rating_weight = rating_weight\n",
    "    self.retrieval_weight = retrieval_weight\n",
    "\n",
    "  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model({\"user_id\": features[\"user_id\"]})\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    movie_embeddings = self.movie_model(\n",
    "        {\"movie_id\": features[\"movie_id\"]})\n",
    "    \n",
    "    return (\n",
    "        user_embeddings,\n",
    "        movie_embeddings,\n",
    "        # We apply the multi-layered rating model to a concatentation of\n",
    "        # user and movie embeddings.\n",
    "        self.rating_model(\n",
    "            tf.concat([user_embeddings, movie_embeddings], axis=1))\n",
    "    )\n",
    "\n",
    "  def train_loss(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "\n",
    "    user_embeddings, movie_embeddings, rating_predictions = self(features)\n",
    "\n",
    "    # We compute the loss for each task.\n",
    "    rating_loss = self.rating_task(labels=features[\"rating\"],\n",
    "                                   predictions=rating_predictions)\n",
    "    retrieval_loss = self.retrieval_task(user_embeddings, movie_embeddings)\n",
    "\n",
    "    # And combine them using the loss weights.\n",
    "    return (self.rating_weight * rating_loss\n",
    "            + self.retrieval_weight * retrieval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngvn-c0b8lc2"
   },
   "source": [
    "### Rating-specialized model\n",
    "\n",
    "Depending on the weights we assign, the model will encode a different balance of the tasks. Let's start with a model that only considers ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 54
    },
    "colab_type": "code",
    "id": "NNfB6rYL0VrS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model = MovielensModel(rating_weight=1.0, retrieval_weight=0.0)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6kjfF1j0iZR"
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 530
    },
    "colab_type": "code",
    "id": "6NWadH1q0c_T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 3s 336ms/step - mean_absolute_error: 2.4425 - factorized_top_k_1: 0.0194 - factorized_top_k_1/top_1_categorical_accuracy: 4.3750e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0027 - factorized_top_k_1/top_10_categorical_accuracy: 0.0055 - factorized_top_k_1/top_50_categorical_accuracy: 0.0295 - factorized_top_k_1/top_100_categorical_accuracy: 0.0585 - loss: 7.3609\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 3s 321ms/step - mean_absolute_error: 0.9370 - factorized_top_k_1: 0.0194 - factorized_top_k_1/top_1_categorical_accuracy: 4.5000e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0027 - factorized_top_k_1/top_10_categorical_accuracy: 0.0056 - factorized_top_k_1/top_50_categorical_accuracy: 0.0296 - factorized_top_k_1/top_100_categorical_accuracy: 0.0585 - loss: 1.2698\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 3s 318ms/step - mean_absolute_error: 0.9273 - factorized_top_k_1: 0.0194 - factorized_top_k_1/top_1_categorical_accuracy: 3.7500e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0028 - factorized_top_k_1/top_10_categorical_accuracy: 0.0056 - factorized_top_k_1/top_50_categorical_accuracy: 0.0297 - factorized_top_k_1/top_100_categorical_accuracy: 0.0586 - loss: 1.2421\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 3s 318ms/step - mean_absolute_error: 0.9212 - factorized_top_k_1: 0.0195 - factorized_top_k_1/top_1_categorical_accuracy: 4.3750e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0027 - factorized_top_k_1/top_10_categorical_accuracy: 0.0058 - factorized_top_k_1/top_50_categorical_accuracy: 0.0297 - factorized_top_k_1/top_100_categorical_accuracy: 0.0590 - loss: 1.2301\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 3s 318ms/step - mean_absolute_error: 0.9158 - factorized_top_k_1: 0.0196 - factorized_top_k_1/top_1_categorical_accuracy: 3.7500e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0028 - factorized_top_k_1/top_10_categorical_accuracy: 0.0056 - factorized_top_k_1/top_50_categorical_accuracy: 0.0299 - factorized_top_k_1/top_100_categorical_accuracy: 0.0591 - loss: 1.2207\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 3s 318ms/step - mean_absolute_error: 0.9105 - factorized_top_k_1: 0.0197 - factorized_top_k_1/top_1_categorical_accuracy: 4.2500e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0028 - factorized_top_k_1/top_10_categorical_accuracy: 0.0058 - factorized_top_k_1/top_50_categorical_accuracy: 0.0301 - factorized_top_k_1/top_100_categorical_accuracy: 0.0592 - loss: 1.2115\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 3s 319ms/step - mean_absolute_error: 0.9052 - factorized_top_k_1: 0.0197 - factorized_top_k_1/top_1_categorical_accuracy: 4.6250e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0029 - factorized_top_k_1/top_10_categorical_accuracy: 0.0058 - factorized_top_k_1/top_50_categorical_accuracy: 0.0302 - factorized_top_k_1/top_100_categorical_accuracy: 0.0594 - loss: 1.2020\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 3s 318ms/step - mean_absolute_error: 0.8997 - factorized_top_k_1: 0.0198 - factorized_top_k_1/top_1_categorical_accuracy: 4.0000e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0029 - factorized_top_k_1/top_10_categorical_accuracy: 0.0058 - factorized_top_k_1/top_50_categorical_accuracy: 0.0304 - factorized_top_k_1/top_100_categorical_accuracy: 0.0596 - loss: 1.1922\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 3s 318ms/step - mean_absolute_error: 0.8940 - factorized_top_k_1: 0.0199 - factorized_top_k_1/top_1_categorical_accuracy: 4.5000e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0029 - factorized_top_k_1/top_10_categorical_accuracy: 0.0058 - factorized_top_k_1/top_50_categorical_accuracy: 0.0306 - factorized_top_k_1/top_100_categorical_accuracy: 0.0597 - loss: 1.1821\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 3s 320ms/step - mean_absolute_error: 0.8883 - factorized_top_k_1: 0.0199 - factorized_top_k_1/top_1_categorical_accuracy: 3.7500e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0029 - factorized_top_k_1/top_10_categorical_accuracy: 0.0059 - factorized_top_k_1/top_50_categorical_accuracy: 0.0306 - factorized_top_k_1/top_100_categorical_accuracy: 0.0598 - loss: 1.1717\n",
      "5/5 [==============================] - 1s 110ms/step - mean_absolute_error: 0.8906 - factorized_top_k_1: 0.0210 - factorized_top_k_1/top_1_categorical_accuracy: 8.0000e-04 - factorized_top_k_1/top_5_categorical_accuracy: 0.0037 - factorized_top_k_1/top_10_categorical_accuracy: 0.0071 - factorized_top_k_1/top_50_categorical_accuracy: 0.0312 - factorized_top_k_1/top_100_categorical_accuracy: 0.0622 - loss: 1.1990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_error': 0.8905865550041199,\n",
       " 'factorized_top_k_1': array([0.0008 , 0.0037 , 0.00715, 0.03115, 0.06225], dtype=float32),\n",
       " 'factorized_top_k_1/top_1_categorical_accuracy': 0.0007999999797903001,\n",
       " 'factorized_top_k_1/top_5_categorical_accuracy': 0.003700000001117587,\n",
       " 'factorized_top_k_1/top_10_categorical_accuracy': 0.007149999961256981,\n",
       " 'factorized_top_k_1/top_50_categorical_accuracy': 0.031150000169873238,\n",
       " 'factorized_top_k_1/top_100_categorical_accuracy': 0.06224999949336052,\n",
       " 'loss': 1.2088614702224731}"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=10)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lENViv04-i0T"
   },
   "source": [
    "The model does OK on predicting ratings (with an MSE of around 0.79), but performs poorly at predicting which movies will be watched or not: its accuracy at 100 is almost 3 times worse than a model trained solely to predict watches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPYd9LtE-4Fm"
   },
   "source": [
    "### Retrieval-specialized model\n",
    "\n",
    "Let's now try a model that focuses on retrieval only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfnkGd2G--Qt"
   },
   "outputs": [],
   "source": [
    "model = MovielensModel(rating_weight=0.0, retrieval_weight=1.0)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 547
    },
    "colab_type": "code",
    "id": "JCCBdM7U_B11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 3s 317ms/step - mean_absolute_error: 3.5752 - factorized_top_k_2: 0.0269 - factorized_top_k_2/top_1_categorical_accuracy: 4.0000e-04 - factorized_top_k_2/top_5_categorical_accuracy: 0.0036 - factorized_top_k_2/top_10_categorical_accuracy: 0.0072 - factorized_top_k_2/top_50_categorical_accuracy: 0.0413 - factorized_top_k_2/top_100_categorical_accuracy: 0.0820 - loss: 70217.8551\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 3s 311ms/step - mean_absolute_error: 3.5976 - factorized_top_k_2: 0.1075 - factorized_top_k_2/top_1_categorical_accuracy: 0.0030 - factorized_top_k_2/top_5_categorical_accuracy: 0.0208 - factorized_top_k_2/top_10_categorical_accuracy: 0.0418 - factorized_top_k_2/top_50_categorical_accuracy: 0.1767 - factorized_top_k_2/top_100_categorical_accuracy: 0.2954 - loss: 67798.1534\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 3s 314ms/step - mean_absolute_error: 3.6113 - factorized_top_k_2: 0.1198 - factorized_top_k_2/top_1_categorical_accuracy: 0.0032 - factorized_top_k_2/top_5_categorical_accuracy: 0.0254 - factorized_top_k_2/top_10_categorical_accuracy: 0.0500 - factorized_top_k_2/top_50_categorical_accuracy: 0.1968 - factorized_top_k_2/top_100_categorical_accuracy: 0.3234 - loss: 66362.1555\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 3s 312ms/step - mean_absolute_error: 3.6203 - factorized_top_k_2: 0.1271 - factorized_top_k_2/top_1_categorical_accuracy: 0.0030 - factorized_top_k_2/top_5_categorical_accuracy: 0.0280 - factorized_top_k_2/top_10_categorical_accuracy: 0.0547 - factorized_top_k_2/top_50_categorical_accuracy: 0.2100 - factorized_top_k_2/top_100_categorical_accuracy: 0.3397 - loss: 65587.9062\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 3s 313ms/step - mean_absolute_error: 3.6272 - factorized_top_k_2: 0.1325 - factorized_top_k_2/top_1_categorical_accuracy: 0.0032 - factorized_top_k_2/top_5_categorical_accuracy: 0.0298 - factorized_top_k_2/top_10_categorical_accuracy: 0.0586 - factorized_top_k_2/top_50_categorical_accuracy: 0.2199 - factorized_top_k_2/top_100_categorical_accuracy: 0.3510 - loss: 65071.0241\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 3s 317ms/step - mean_absolute_error: 3.6328 - factorized_top_k_2: 0.1364 - factorized_top_k_2/top_1_categorical_accuracy: 0.0030 - factorized_top_k_2/top_5_categorical_accuracy: 0.0311 - factorized_top_k_2/top_10_categorical_accuracy: 0.0611 - factorized_top_k_2/top_50_categorical_accuracy: 0.2276 - factorized_top_k_2/top_100_categorical_accuracy: 0.3593 - loss: 64692.7237\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 3s 316ms/step - mean_absolute_error: 3.6373 - factorized_top_k_2: 0.1394 - factorized_top_k_2/top_1_categorical_accuracy: 0.0029 - factorized_top_k_2/top_5_categorical_accuracy: 0.0323 - factorized_top_k_2/top_10_categorical_accuracy: 0.0640 - factorized_top_k_2/top_50_categorical_accuracy: 0.2323 - factorized_top_k_2/top_100_categorical_accuracy: 0.3654 - loss: 64401.3125\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 3s 317ms/step - mean_absolute_error: 3.6411 - factorized_top_k_2: 0.1414 - factorized_top_k_2/top_1_categorical_accuracy: 0.0026 - factorized_top_k_2/top_5_categorical_accuracy: 0.0329 - factorized_top_k_2/top_10_categorical_accuracy: 0.0654 - factorized_top_k_2/top_50_categorical_accuracy: 0.2371 - factorized_top_k_2/top_100_categorical_accuracy: 0.3692 - loss: 64168.7479\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 3s 317ms/step - mean_absolute_error: 3.6442 - factorized_top_k_2: 0.1431 - factorized_top_k_2/top_1_categorical_accuracy: 0.0023 - factorized_top_k_2/top_5_categorical_accuracy: 0.0334 - factorized_top_k_2/top_10_categorical_accuracy: 0.0667 - factorized_top_k_2/top_50_categorical_accuracy: 0.2396 - factorized_top_k_2/top_100_categorical_accuracy: 0.3734 - loss: 63978.1413\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 3s 320ms/step - mean_absolute_error: 3.6470 - factorized_top_k_2: 0.1447 - factorized_top_k_2/top_1_categorical_accuracy: 0.0026 - factorized_top_k_2/top_5_categorical_accuracy: 0.0345 - factorized_top_k_2/top_10_categorical_accuracy: 0.0676 - factorized_top_k_2/top_50_categorical_accuracy: 0.2421 - factorized_top_k_2/top_100_categorical_accuracy: 0.3768 - loss: 63818.5511\n",
      "5/5 [==============================] - 1s 117ms/step - mean_absolute_error: 3.6426 - factorized_top_k_2: 0.0502 - factorized_top_k_2/top_1_categorical_accuracy: 1.0000e-04 - factorized_top_k_2/top_5_categorical_accuracy: 0.0025 - factorized_top_k_2/top_10_categorical_accuracy: 0.0071 - factorized_top_k_2/top_50_categorical_accuracy: 0.0756 - factorized_top_k_2/top_100_categorical_accuracy: 0.1657 - loss: 31882.4714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_error': 3.642604351043701,\n",
       " 'factorized_top_k_2': array([1.0000e-04, 2.5000e-03, 7.1000e-03, 7.5650e-02, 1.6575e-01],\n",
       "       dtype=float32),\n",
       " 'factorized_top_k_2/top_1_categorical_accuracy': 9.999999747378752e-05,\n",
       " 'factorized_top_k_2/top_5_categorical_accuracy': 0.0024999999441206455,\n",
       " 'factorized_top_k_2/top_10_categorical_accuracy': 0.0071000000461936,\n",
       " 'factorized_top_k_2/top_50_categorical_accuracy': 0.07564999908208847,\n",
       " 'factorized_top_k_2/top_100_categorical_accuracy': 0.16574999690055847,\n",
       " 'loss': 29010.5}"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=10)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjM7j7eY_jPh"
   },
   "source": [
    "We get the opposite result: a model that does well on retrieval, but poorly on predicting ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOFwjUus_pLU"
   },
   "source": [
    "### Equally-weighted model\n",
    "\n",
    "Let's now train a model that weighs both tasks equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xyDbNMf_t8a"
   },
   "outputs": [],
   "source": [
    "model = MovielensModel(rating_weight=1.0, retrieval_weight=1.0)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 530
    },
    "colab_type": "code",
    "id": "2pZmM_ub_uEO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 3s 319ms/step - mean_absolute_error: 2.3338 - factorized_top_k_3: 0.0263 - factorized_top_k_3/top_1_categorical_accuracy: 3.1250e-04 - factorized_top_k_3/top_5_categorical_accuracy: 0.0031 - factorized_top_k_3/top_10_categorical_accuracy: 0.0070 - factorized_top_k_3/top_50_categorical_accuracy: 0.0402 - factorized_top_k_3/top_100_categorical_accuracy: 0.0807 - loss: 70222.5533\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 3s 316ms/step - mean_absolute_error: 0.9475 - factorized_top_k_3: 0.1010 - factorized_top_k_3/top_1_categorical_accuracy: 0.0027 - factorized_top_k_3/top_5_categorical_accuracy: 0.0191 - factorized_top_k_3/top_10_categorical_accuracy: 0.0388 - factorized_top_k_3/top_50_categorical_accuracy: 0.1652 - factorized_top_k_3/top_100_categorical_accuracy: 0.2793 - loss: 67918.7834\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 3s 317ms/step - mean_absolute_error: 0.9153 - factorized_top_k_3: 0.1188 - factorized_top_k_3/top_1_categorical_accuracy: 0.0031 - factorized_top_k_3/top_5_categorical_accuracy: 0.0236 - factorized_top_k_3/top_10_categorical_accuracy: 0.0484 - factorized_top_k_3/top_50_categorical_accuracy: 0.1964 - factorized_top_k_3/top_100_categorical_accuracy: 0.3223 - loss: 66485.0227\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 3s 314ms/step - mean_absolute_error: 0.8902 - factorized_top_k_3: 0.1280 - factorized_top_k_3/top_1_categorical_accuracy: 0.0030 - factorized_top_k_3/top_5_categorical_accuracy: 0.0276 - factorized_top_k_3/top_10_categorical_accuracy: 0.0542 - factorized_top_k_3/top_50_categorical_accuracy: 0.2132 - factorized_top_k_3/top_100_categorical_accuracy: 0.3418 - loss: 65658.0092\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 3s 316ms/step - mean_absolute_error: 0.8767 - factorized_top_k_3: 0.1340 - factorized_top_k_3/top_1_categorical_accuracy: 0.0029 - factorized_top_k_3/top_5_categorical_accuracy: 0.0297 - factorized_top_k_3/top_10_categorical_accuracy: 0.0586 - factorized_top_k_3/top_50_categorical_accuracy: 0.2236 - factorized_top_k_3/top_100_categorical_accuracy: 0.3550 - loss: 65104.6520\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 3s 320ms/step - mean_absolute_error: 0.8627 - factorized_top_k_3: 0.1386 - factorized_top_k_3/top_1_categorical_accuracy: 0.0030 - factorized_top_k_3/top_5_categorical_accuracy: 0.0312 - factorized_top_k_3/top_10_categorical_accuracy: 0.0620 - factorized_top_k_3/top_50_categorical_accuracy: 0.2317 - factorized_top_k_3/top_100_categorical_accuracy: 0.3652 - loss: 64703.4396\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 3s 314ms/step - mean_absolute_error: 0.8516 - factorized_top_k_3: 0.1417 - factorized_top_k_3/top_1_categorical_accuracy: 0.0029 - factorized_top_k_3/top_5_categorical_accuracy: 0.0324 - factorized_top_k_3/top_10_categorical_accuracy: 0.0647 - factorized_top_k_3/top_50_categorical_accuracy: 0.2372 - factorized_top_k_3/top_100_categorical_accuracy: 0.3711 - loss: 64397.0746\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 3s 316ms/step - mean_absolute_error: 0.8435 - factorized_top_k_3: 0.1438 - factorized_top_k_3/top_1_categorical_accuracy: 0.0028 - factorized_top_k_3/top_5_categorical_accuracy: 0.0330 - factorized_top_k_3/top_10_categorical_accuracy: 0.0666 - factorized_top_k_3/top_50_categorical_accuracy: 0.2409 - factorized_top_k_3/top_100_categorical_accuracy: 0.3756 - loss: 64154.4446\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 3s 321ms/step - mean_absolute_error: 0.8367 - factorized_top_k_3: 0.1456 - factorized_top_k_3/top_1_categorical_accuracy: 0.0027 - factorized_top_k_3/top_5_categorical_accuracy: 0.0340 - factorized_top_k_3/top_10_categorical_accuracy: 0.0680 - factorized_top_k_3/top_50_categorical_accuracy: 0.2441 - factorized_top_k_3/top_100_categorical_accuracy: 0.3794 - loss: 63956.8793\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 3s 316ms/step - mean_absolute_error: 0.8310 - factorized_top_k_3: 0.1471 - factorized_top_k_3/top_1_categorical_accuracy: 0.0027 - factorized_top_k_3/top_5_categorical_accuracy: 0.0345 - factorized_top_k_3/top_10_categorical_accuracy: 0.0689 - factorized_top_k_3/top_50_categorical_accuracy: 0.2467 - factorized_top_k_3/top_100_categorical_accuracy: 0.3826 - loss: 63792.3714\n",
      "5/5 [==============================] - 1s 117ms/step - mean_absolute_error: 0.8153 - factorized_top_k_3: 0.0507 - factorized_top_k_3/top_1_categorical_accuracy: 2.0000e-04 - factorized_top_k_3/top_5_categorical_accuracy: 0.0022 - factorized_top_k_3/top_10_categorical_accuracy: 0.0073 - factorized_top_k_3/top_50_categorical_accuracy: 0.0746 - factorized_top_k_3/top_100_categorical_accuracy: 0.1693 - loss: 31867.0755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_error': 0.815298855304718,\n",
       " 'factorized_top_k_3': array([0.0002 , 0.00225, 0.0073 , 0.07455, 0.1693 ], dtype=float32),\n",
       " 'factorized_top_k_3/top_1_categorical_accuracy': 0.00019999999494757503,\n",
       " 'factorized_top_k_3/top_5_categorical_accuracy': 0.0022499999031424522,\n",
       " 'factorized_top_k_3/top_10_categorical_accuracy': 0.007300000172108412,\n",
       " 'factorized_top_k_3/top_50_categorical_accuracy': 0.07455000281333923,\n",
       " 'factorized_top_k_3/top_100_categorical_accuracy': 0.16930000483989716,\n",
       " 'loss': 28973.927734375}"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=10)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ni_rkOsaB3f9"
   },
   "source": [
    "The result is a model that does better on both tasks. This is the promise of transfer learning, where jointly training a model on a set of related tasks can result in a better model that training a set of separate estimators.\n",
    "\n",
    "Of course, this is not true everywhere. We can expect better results when tasks are closely related, or when we can trasnfer knowledge from a data-abundant task (such as clicks) to a related data-sparse task (such as purchases)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "multitask.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
