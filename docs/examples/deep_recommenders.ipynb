{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ChjuaQjm_iBf"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWqCArLO_kez"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ikhIvrku-i-L"
   },
   "source": [
    "# Using deep models in retrieval\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/deep_recommenders\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "In [the featurization tutorial](https://github.com/tensorflow/recommenders/blob/main/docs/examples/featurization.ipynb) we incorporated multiple features into our models, but the models consist of only an embedding layer. We can add more dense layers to our models to increase their capacity.\n",
    "\n",
    "Deep models with multiple layers can approximate more complex patterns and functions than models with only an embedding layer. Furthermore, with more layers, the learnability of the model might also improve. While model with one hidden layer can approximate any function in theory, in practice models with more hidden layers can learn to approximate complex functions more easily.\n",
    "\n",
    "Nonetheless, complex models also have their disadvantages. More layers require more training epochs, and each training step would require more computation. It would also be harder for the gradients to propagate through models with more layers when updating model parameters. Furthermore, with more parameters, deep models might overfit or even simply memorize the training examples instead of learning a function that can generalize.\n",
    "\n",
    "In this notebook we will build deep models with multiple layers and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7RYXwgbAcbU"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "We first import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbwMjnLP5nZ_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgKIjpQLAiax"
   },
   "source": [
    "In this tutorial we will use the models from [the featurization tutorial](https://github.com/tensorflow/recommenders/blob/main/docs/examples/featurization.ipynb) to generate embeddings. Hence we will only be using the user id, timestamp, and movie title features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kc2REbOO52Fl"
   },
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"timestamp\": x[\"timestamp\"],\n",
    "})\n",
    "movies = movies.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YZ2q5RXYNI6"
   },
   "source": [
    "We bucketize the timestamp feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5CVveCS9Doq"
   },
   "outputs": [],
   "source": [
    "max_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
    "min_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    np.int64(1e9), tf.minimum).numpy().min()\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFJcCVMUQou3"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtS6a4sgmI-c"
   },
   "source": [
    "### Query model\n",
    "\n",
    "We will define a generic query model that can have different architectures depending on the constructor arguments.\n",
    "\n",
    "We will use the user model defined in [the featurization tutorial](https://github.com/tensorflow/recommenders/blob/main/docs/examples/featurization.ipynb) as a component of our query model. It will convert input examples into feature embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ItzYwMW42cb"
   },
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    num_hashing_bins = 20_000\n",
    "\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Hashing(num_bins=num_hashing_bins),\n",
    "        tf.keras.layers.Embedding(num_hashing_bins, 32),\n",
    "    ])\n",
    "    self.timestamp_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "        tf.keras.layers.Embedding(len(timestamp_buckets) + 2, 32),\n",
    "    ])\n",
    "    self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    # Take the input dictionary, pass it through each input layer,\n",
    "    # and concatenate the result.\n",
    "    return tf.concat([\n",
    "        self.user_embedding(inputs[\"user_id\"]),\n",
    "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "        self.normalized_timestamp(inputs[\"timestamp\"]),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMQzxLqh42on"
   },
   "source": [
    "In addition to the embedding model, we also add hidden layers according to the argument `hidden_layer_sizes` to make the query model deep. Since deep linear models have the same expressive power as normal linear models, we use ReLUs for all hidden layers to allow to model nonlinearities. After adding the hidden layers, we add a projection layer to generate embeddings of dimensionality specified by `final_embedding_dimension`.\n",
    "\n",
    "Note that we do not use any activation function on the projection layer. Using an activation function would limit the output space of the final embeddings and might negatively impact the performance of the model. For instance, if ReLUs are used in the projection layer, all components in the output embedding would be non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qfPi4I-Z0ph"
   },
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding user queries.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      final_embedding_dimension,\n",
    "      hidden_layer_sizes=None,\n",
    "  ):\n",
    "    \"\"\"Initializes a query model for encoding user queries.\n",
    "\n",
    "    Args:\n",
    "      final_embedding_dimension:\n",
    "        An integer representing the dimensionality of the final embedding. The\n",
    "        model would add a final projection layer with\n",
    "        `final_embedding_dimension` units to ensure that the output embedding\n",
    "        has the specified number of dimensions.\n",
    "      hidden_layer_sizes:\n",
    "        A list of integers where the ith entry represents the number of units\n",
    "        the ith layer contains.\n",
    "    \n",
    "    Returns:\n",
    "      A query model for encoding queries.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    if hidden_layer_sizes is None:\n",
    "      hidden_layer_sizes = []\n",
    "\n",
    "    # We first use the user model for generating embeddings.\n",
    "    self.embedding_model = UserModel()\n",
    "\n",
    "    dense_layer_list = []\n",
    "    # We now add the hidden layers.\n",
    "    for layer_size in hidden_layer_sizes:\n",
    "      dense_layer_list.append(\n",
    "          tf.keras.layers.Dense(layer_size, activation=\"relu\"),\n",
    "      )\n",
    "    # We finally add a projection layer without any activation function.\n",
    "    dense_layer_list.append(\n",
    "        tf.keras.layers.Dense(final_embedding_dimension, activation=None),\n",
    "    )\n",
    "    self.dense_layers = tf.keras.Sequential(dense_layer_list, name='dense_layers')\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XleMceZNHC__"
   },
   "source": [
    "### Candidate model\n",
    "\n",
    "Since we are focusing on exploring different query models, we will keep the candidate model simple and use only the embedding model from [the featurization tutorial](https://github.com/tensorflow/recommenders/blob/main/docs/examples/featurization.ipynb). Note that this model generates embeddings with 64 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQZHX8bEHPOk"
   },
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    num_hashing_bins = 20_000\n",
    "    max_tokens = 10_000\n",
    "\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.Hashing(num_bins=num_hashing_bins),\n",
    "      tf.keras.layers.Embedding(num_hashing_bins, 32)\n",
    "    ])\n",
    "    self.title_text_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_tokens),\n",
    "      tf.keras.layers.Embedding(max_tokens, 32)\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.concat([\n",
    "        self.title_embedding(inputs[\"movie_title\"]),\n",
    "        # We average the embedding of individual words to get one embedding vector\n",
    "        # per title.\n",
    "        tf.reduce_mean(self.title_text_embedding(inputs[\"movie_title\"]), axis=-2),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cc4KbTNwHSvD"
   },
   "source": [
    "### Combined model\n",
    "\n",
    "We now define a combined model that takes a query model and a candidate model as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26_hNJPKIh4-"
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, query_model, candidate_model):\n",
    "    super().__init__()\n",
    "    self.query_model: tf.keras.Model = query_model\n",
    "    self.candidate_model: tf.keras.Model = candidate_model\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "        metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=movies.batch(128).map(self.candidate_model),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    # We only pass the user id and timestamp features into the query model. This\n",
    "    # is to ensure that the training inputs would have the same keys as the\n",
    "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "    # error when loading the query model after saving it.\n",
    "    query_embeddings = self.query_model({\n",
    "        \"user_id\": features[\"user_id\"],\n",
    "        \"timestamp\": features[\"timestamp\"],\n",
    "    })\n",
    "    positive_movie_embeddings = self.candidate_model({\n",
    "        \"movie_title\": features[\"movie_title\"],\n",
    "    })\n",
    "    return self.task(query_embeddings, positive_movie_embeddings, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YXjsRsLTVzt"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QY7MTwMruoKh"
   },
   "source": [
    "### Prepare the data\n",
    "\n",
    "We first split the data into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMFUZ4dyTdYd"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2HEuTBzJ9w5"
   },
   "source": [
    "### Model with one dense layer\n",
    "\n",
    "We now define the dimensionalities of the final embeddings below. Since `MovieModel` generates embeddings with 64 dimensions, we want our query models to generate 64 dimensional embeddings as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xysGAeOyQd0U"
   },
   "outputs": [],
   "source": [
    "final_embedding_dimension = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WECDQLtQzts"
   },
   "source": [
    "We now define a simple model with only a projection layer and no hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkoLkiQdK4Um"
   },
   "outputs": [],
   "source": [
    "one_layer_query_model = QueryModel(\n",
    "    final_embedding_dimension,\n",
    ")\n",
    "one_layer_candidate_model = MovieModel()\n",
    "one_layer_candidate_model.title_text_embedding.layers[0].adapt(\n",
    "    movies.map(lambda x: x[\"movie_title\"]),\n",
    ")\n",
    "one_layer_model = MovielensModel(\n",
    "    one_layer_query_model,\n",
    "    one_layer_candidate_model,\n",
    ")\n",
    "one_layer_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LicXTr1vLA-L"
   },
   "outputs": [],
   "source": [
    "one_layer_model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjJ1anzuLXgN"
   },
   "source": [
    "### Model with three dense layers and no activation function\n",
    "\n",
    "We now create and train a model with two hidden layers of size 64 and one projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "11qAr5gGMUxE"
   },
   "outputs": [],
   "source": [
    "three_layer_query_model = QueryModel(\n",
    "    final_embedding_dimension,\n",
    "    hidden_layer_sizes=[64, 64],\n",
    ")\n",
    "three_layer_candidate_model = MovieModel()\n",
    "three_layer_candidate_model.title_text_embedding.layers[0].adapt(\n",
    "    movies.map(lambda x: x[\"movie_title\"]),\n",
    ")\n",
    "three_layer_model = MovielensModel(\n",
    "    three_layer_query_model,\n",
    "    three_layer_candidate_model,\n",
    ")\n",
    "three_layer_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nqR-97yk63H"
   },
   "outputs": [],
   "source": [
    "three_layer_model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwNey8NJpsqH"
   },
   "source": [
    "### Model with five dense layers and no activation function\n",
    "\n",
    "We now create a model with four hidden layers of sizes 128, 128, 64, 64, and a projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vi-KduIepzUf"
   },
   "outputs": [],
   "source": [
    "five_layer_query_model = QueryModel(\n",
    "    final_embedding_dimension,\n",
    "    hidden_layer_sizes=[128, 128, 64, 64],\n",
    ")\n",
    "five_layer_candidate_model = MovieModel()\n",
    "five_layer_candidate_model.title_text_embedding.layers[0].adapt(\n",
    "    movies.map(lambda x: x[\"movie_title\"]),\n",
    ")\n",
    "five_layer_model = MovielensModel(\n",
    "    five_layer_query_model,\n",
    "    five_layer_candidate_model,\n",
    ")\n",
    "five_layer_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HqFBowJ0p7Pr"
   },
   "outputs": [],
   "source": [
    "five_layer_model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Yw1PeInSh8h"
   },
   "source": [
    "## Comparing the models\n",
    "\n",
    "We now evaluate the four models and compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_g-hcYd_Beqp"
   },
   "outputs": [],
   "source": [
    "one_layer_results = one_layer_model.evaluate(cached_test, return_dict=True)\n",
    "print(\"Top 100 categorical accuracy: {:.4f}\".format(one_layer_results[\"factorized_top_k/top_100_categorical_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "js2RjZQnozB0"
   },
   "outputs": [],
   "source": [
    "three_layer_results = three_layer_model.evaluate(cached_test, return_dict=True)\n",
    "print(\"Top 100 categorical accuracy: {:.4f}\".format(three_layer_results[\"factorized_top_k/top_100_categorical_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9KUQM4Do0n4"
   },
   "outputs": [],
   "source": [
    "five_layer_results = five_layer_model.evaluate(cached_test, return_dict=True)\n",
    "print(\"Top 100 categorical accuracy: {:.4f}\".format(five_layer_results[\"factorized_top_k/top_100_categorical_accuracy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUNC9D70vhrp"
   },
   "source": [
    "We focus on the top 100 categorical accuracy. In many recommender systems, a retrieval model picks candidates for ranking models. The number of candidates should be large enough to ensure items of interest are included. Hence we look at the top 100 candidates returned by each model here.\n",
    "\n",
    "The one layer model has the highest top 100 categorical accuracy. This shows that a more complex model does not always guarantee better performance. As mentioned above, complex models require more training epochs. In this case, three epochs might not be sufficient for a three layer or five layer model. Complex models also require more regularization, which is not used in this example.\n",
    "\n",
    "Of course, all these results should be treated with a certain level of skepticism. We did not thoroughly tune all hyperparameters, including the learning rate, the optimizer, and regularization. These hyperparameters may play a huge role in model performance, and we cannot have conclusive results without fully tuning these hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTJeAkgcxhF0"
   },
   "source": [
    "## Serving the model\n",
    "\n",
    "We now serve our best performing model, the one layer model. In a simple recommendation model, when receiving a query, we simply compute the embedding of the query and find the movies with the closest embeddings to that query. For efficiency, we can precompute the embeddings of all candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb3vp9NSh9hf"
   },
   "outputs": [],
   "source": [
    "movie_embeddings = movies.batch(1_000).map(lambda x: one_layer_model.candidate_model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GV7_1LeziTWl"
   },
   "source": [
    "To create a recommendation model that takes a query and returns the top candidates, we use the `ann` module in TFRS. Since the number of movies in the dataset is small, we can use the `BruteForce` layer to look for top candidates. We pass in the query model as an argument for encoding input queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Pr0t8HhfITF"
   },
   "outputs": [],
   "source": [
    "serving_model = tfrs.layers.ann.BruteForce(one_layer_model.query_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MalSkARrjZw_"
   },
   "source": [
    "We then let the recommendation model index the candidates by calling `index` and pass in the movie embeddings we precomputed and the movie titles as identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wHlLW_6jjt0"
   },
   "outputs": [],
   "source": [
    "serving_model.index(\n",
    "    candidates=movie_embeddings,\n",
    "    identifiers=movies.batch(1_000).map(lambda x: x[\"movie_title\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ai5jg3QNqaPh"
   },
   "source": [
    "We can query the model for recommendations by passing in a dictionary of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JipvUA_nqZk8"
   },
   "outputs": [],
   "source": [
    "scores, titles = serving_model(\n",
    "    {\"user_id\": np.array([\"42\"]), \"timestamp\": np.array([879024327])},\n",
    "    num_candidates=3,\n",
    ")\n",
    "for i, title in enumerate(titles[0].numpy().tolist()):\n",
    "  print(\"{:d}. {:s}\".format(i + 1, str(title)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqAFMrx7pua3"
   },
   "source": [
    "We can save the model using `tf.saved_model.save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIbfl_Q4qE2h"
   },
   "outputs": [],
   "source": [
    "tmp = tempfile.TemporaryDirectory()\n",
    "path = os.path.join(tmp.name, \"s_model\")\n",
    "tf.saved_model.save(serving_model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_y7kJfcqh1X"
   },
   "source": [
    "We can then load the model using `tf.keras.models.load_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27-o0wePqgnJ"
   },
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHv-AQbi-wl0"
   },
   "source": [
    "We can query the loaded model for recommendations by passing in a dictionary of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ys5fl6brnS7C"
   },
   "outputs": [],
   "source": [
    "scores, titles = loaded_model(\n",
    "    {\"user_id\": np.array([\"42\"]), \"timestamp\": np.array([879024327])}\n",
    ")\n",
    "for i, title in enumerate(titles[0].numpy().tolist()):\n",
    "  print(\"{:d}. {:s}\".format(i + 1, str(title)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dB09crfpgBx7"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "In this tutorial we expanded our retrieval model with dense layers and activation functions. To see how to create a model that can perform not only retrieval tasks but also rating tasks, take a look at [the multitask tutorial](https://github.com/tensorflow/recommenders/blob/main/docs/examples/multitask.ipynb)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deep_recommenders.ipynb",
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
